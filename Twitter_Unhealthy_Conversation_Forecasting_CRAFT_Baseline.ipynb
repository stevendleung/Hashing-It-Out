{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Unhealthy Conversation Forecasting- CRAFT Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5RFveG8GfWd"
      },
      "source": [
        "# CRAFT fine-tuning for Twitter Conversation Forecasting\n",
        "\n",
        "This notebook walks through fine-tuning the CRAFT model proposed by Chang et al on our Twitter Conversation data. We are using this model as the baseline for our task of forecasting personal attacks in Twitter conversations.\n",
        "\n",
        "Credit for this base code goes to the authors of \"Trouble on the Horizon\" paper. See notebook below for original fine-tuning process.\n",
        "\n",
        "https://colab.research.google.com/drive/1SH4iMEHdoH4IovN-b9QOSK4kG4DhAwmb#scrollTo=A-MyiLgJnkle\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffvJPbqjdPMT"
      },
      "source": [
        "# start by installing ConvoKit on the colab VM\n",
        "!pip install -q convokit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EraNV9c4v9cW"
      },
      "source": [
        "# import necessary libraries, including convokit\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import requests\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import unicodedata\n",
        "import itertools\n",
        "from urllib.request import urlretrieve\n",
        "from convokit import download, Corpus\n",
        "import io\n",
        "import re\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ULDhltsGSW0"
      },
      "source": [
        "# Loading Our Convo Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G94iZRmsNbBy",
        "outputId": "68f17357-7231-45b5-f76f-08b55ecdc19b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86X6VgRe37Px"
      },
      "source": [
        "corpus_train_path = \"/content/drive/My Drive/data_twitter/train_data_baseline_2021-04-04_v1.csv\"\n",
        "corpus_val_path = \"/content/drive/My Drive/data_twitter/val_data_baseline_2021-04-04_v1.csv\"\n",
        "corpus_test_path = \"/content/drive/My Drive/data_twitter/test_data_baseline_2021-04-04_v1.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "racTVc-iN8pg"
      },
      "source": [
        "corpus_train = pd.read_csv(corpus_train_path)\n",
        "corpus_val = pd.read_csv(corpus_val_path)\n",
        "corpus_test = pd.read_csv(corpus_test_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXaWdczhx1dy"
      },
      "source": [
        "# define globals and constants\n",
        "\n",
        "MAX_LENGTH = 130  # Maximum sentence length (number of tokens) to consider\n",
        "\n",
        "# configure model\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "context_encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 1e-5\n",
        "decoder_learning_ratio = 5.0\n",
        "print_every = 10\n",
        "train_epochs = 5\n",
        "\n",
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "UNK_token = 3  # Unknown word token\n",
        "\n",
        "# model download paths\n",
        "WORD2INDEX_URL = \"http://zissou.infosci.cornell.edu/convokit/models/craft_wikiconv/word2index.json\"\n",
        "INDEX2WORD_URL = \"http://zissou.infosci.cornell.edu/convokit/models/craft_wikiconv/index2word.json\"\n",
        "MODEL_URL = \"http://zissou.infosci.cornell.edu/convokit/models/craft_wikiconv/craft_pretrained.tar\"\n",
        "\n",
        "# confidence score threshold for declaring a positive prediction.\n",
        "# this value was previously learned on the validation set.\n",
        "FORECAST_THRESH = 0.570617"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjdOZi8FHmFm"
      },
      "source": [
        "## Part 1: set up data conversion utilities \n",
        "\n",
        "We begin by setting up some helper functions and classes for converting conversational text data into a torch-friendly Tensor format. Note that these low-level routines are largely taken from the [PyTorch seq2seq chatbot tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eeiz8j0qSSEt"
      },
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    \"\"\"A class for representing the vocabulary used by a CRAFT model\"\"\"\n",
        "\n",
        "    def __init__(self, name, word2index=None, index2word=None):\n",
        "        self.name = name\n",
        "        self.trimmed = False if not word2index else True # if a precomputed vocab is specified assume the user wants to use it as-is\n",
        "        self.word2index = word2index if word2index else {\"UNK\": UNK_token}\n",
        "        self.word2count = {}\n",
        "        self.index2word = index2word if index2word else {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
        "        self.num_words = 4 if not index2word else len(index2word)  # Count SOS, EOS, PAD, UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Remove words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # Reinitialize dictionaries\n",
        "        self.word2index = {\"UNK\": UNK_token}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
        "        self.num_words = 4 # Count default tokens\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n",
        "\n",
        "# Create a Voc object from precomputed data structures\n",
        "def loadPrecomputedVoc(corpus_name, word2index_url, index2word_url):\n",
        "    # load the word-to-index lookup map\n",
        "    r = requests.get(word2index_url)\n",
        "    word2index = r.json()\n",
        "    # load the index-to-word lookup map\n",
        "    r = requests.get(index2word_url)\n",
        "    index2word = r.json()\n",
        "    return Voc(corpus_name, word2index, index2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxp-KumUHCgJ"
      },
      "source": [
        "\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    #Take out user handles and replace with nothing. Do we want to instead \n",
        "    #make this a user token?\n",
        "    s = re.sub('@[^\\s]+','',s)\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# Read query/response pairs and return a voc object\n",
        "def readVocs(datafile, corpus_name):\n",
        "    formattedCorpus = []\n",
        "    for index, value in corpus['Tweet'].items():\n",
        "      formattedCorpus.append(normalizeString(value))\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, formattedCorpus\n",
        "\n",
        "\n",
        "# Using the functions defined above, return a populated voc object and pairs list\n",
        "def loadPrepareData(corpus_name, dataframe):\n",
        "    voc, formattedCorpus = readVocs(dataframe, corpus_name)\n",
        "\n",
        "    for tweet in formattedCorpus:\n",
        "        voc.addSentence(tweet)\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc\n",
        "\n",
        "\n",
        "# # Load/Assemble voc and pairs\n",
        "# save_dir = os.path.join(\"data\", \"save\")\n",
        "# voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
        "# # Print some pairs to validate\n",
        "# print(\"\\npairs:\")\n",
        "# for pair in pairs[:10]:\n",
        "#     print(pair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqzYdb2xSTeO"
      },
      "source": [
        "# Helper functions for preprocessing and tokenizing text\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "\n",
        "# Tokenize the string using NLTK\n",
        "def tokenize(text):\n",
        "    tokenizer = nltk.tokenize.RegexpTokenizer(pattern=r'\\w+|[^\\w\\s]')\n",
        "    # simplify the problem space by considering only ASCII data\n",
        "    cleaned_text = unicodeToAscii(text.lower())\n",
        "\n",
        "    # if the resulting string is empty, nothing else to do\n",
        "    if not cleaned_text.strip():\n",
        "        return []\n",
        "    \n",
        "    return tokenizer.tokenize(cleaned_text)\n",
        "\n",
        "def processDialog(voc, row, index):\n",
        "\n",
        "    context_list = []\n",
        "    \n",
        "    full_context = [row['context_1'], row['context_2']]\n",
        "    \n",
        "    for context in full_context:\n",
        "\n",
        "    \n",
        "      #for tweet in context:\n",
        "      context_tokens = tokenize(context)\n",
        "\n",
        "      # replace out-of-vocabulary tokens\n",
        "      for i in range(len(context_tokens)):\n",
        "        if context_tokens[i] not in voc.word2index:\n",
        "            context_tokens[i] = \"UNK\"\n",
        "      context_list.append(context_tokens)\n",
        "\n",
        "    #for reply:\n",
        "    reply_tokens = tokenize(row['replies'])\n",
        "\n",
        "    # replace out-of-vocabulary tokens\n",
        "    for i in range(len(reply_tokens)):\n",
        "      if reply_tokens[i] not in voc.word2index:\n",
        "          reply_tokens[i] = \"UNK\"\n",
        "\n",
        "    \n",
        "    processed = {\"tokens\": context_list, \"reply\": reply_tokens, \"is_attack\": int(row['labels']), \"id\": index}\n",
        "    \n",
        "    return processed\n",
        "\n",
        "def loadPairs(voc, corpus, split=None, last_only=False):\n",
        "    pairs = []\n",
        "\n",
        "    for index, row in corpus.iterrows():\n",
        "      processed = processDialog(voc, row, index)\n",
        "\n",
        "      context = processed['tokens']\n",
        "      reply = processed['reply']\n",
        "      label = processed['is_attack']\n",
        "      comment_id = processed['id']\n",
        "\n",
        "      pairs.append((context, reply, label, comment_id))\n",
        "\n",
        "    return pairs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjSPPlANX0yg"
      },
      "source": [
        "# Helper functions for turning dialog and text sequences into tensors, and manipulating those tensors\n",
        "\n",
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence] + [EOS_token]\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "# Takes a batch of dialogs (lists of lists of tokens) and converts it into a\n",
        "# batch of utterances (lists of tokens) sorted by length, while keeping track of\n",
        "# the information needed to reconstruct the original batch of dialogs\n",
        "def dialogBatch2UtteranceBatch(dialog_batch):\n",
        "    utt_tuples = [] # will store tuples of (utterance, original position in batch, original position in dialog)\n",
        "    for batch_idx in range(len(dialog_batch)):\n",
        "        dialog = dialog_batch[batch_idx]\n",
        "        for dialog_idx in range(len(dialog)):\n",
        "            utterance = dialog[dialog_idx]\n",
        "            utt_tuples.append((utterance, batch_idx, dialog_idx))\n",
        "    # sort the utterances in descending order of length, to remain consistent with pytorch padding requirements\n",
        "    utt_tuples.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    # return the utterances, original batch indices, and original dialog indices as separate lists\n",
        "    utt_batch = [u[0] for u in utt_tuples]\n",
        "    batch_indices = [u[1] for u in utt_tuples]\n",
        "    dialog_indices = [u[2] for u in utt_tuples]\n",
        "    return utt_batch, batch_indices, dialog_indices\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.ByteTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "# Returns all items for a given batch of pairs\n",
        "def batch2TrainData(voc, pair_batch, already_sorted=False):\n",
        "    if not already_sorted:\n",
        "        pair_batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    input_batch, output_batch, label_batch, id_batch = [], [], [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "        label_batch.append(pair[2])\n",
        "        id_batch.append(pair[3])\n",
        "    dialog_lengths = torch.tensor([len(x) for x in input_batch])\n",
        "    input_utterances, batch_indices, dialog_indices = dialogBatch2UtteranceBatch(input_batch)\n",
        "    inp, utt_lengths = inputVar(input_utterances, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    label_batch = torch.FloatTensor(label_batch) if label_batch[0] is not None else None\n",
        "    return inp, dialog_lengths, utt_lengths, batch_indices, dialog_indices, label_batch, id_batch, output, mask, max_target_len\n",
        "\n",
        "def batchIterator(voc, source_data, batch_size, shuffle=True):\n",
        "    cur_idx = 0\n",
        "    if shuffle:\n",
        "        random.shuffle(source_data)\n",
        "    while True:\n",
        "        if cur_idx >= len(source_data):\n",
        "            cur_idx = 0\n",
        "            if shuffle:\n",
        "                random.shuffle(source_data)\n",
        "        batch = source_data[cur_idx:(cur_idx+batch_size)]\n",
        "        # the true batch size may be smaller than the given batch size if there is not enough data left\n",
        "        true_batch_size = len(batch)\n",
        "        # ensure that the dialogs in this batch are sorted by length, as expected by the padding module\n",
        "        batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "        # for analysis purposes, get the source dialogs and labels associated with this batch\n",
        "        batch_dialogs = [x[0] for x in batch]\n",
        "        batch_labels = [x[2] for x in batch]\n",
        "        # convert batch to tensors\n",
        "        batch_tensors = batch2TrainData(voc, batch, already_sorted=True)\n",
        "        yield (batch_tensors, batch_dialogs, batch_labels, true_batch_size) \n",
        "        cur_idx += batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUhEjfRqIwcO"
      },
      "source": [
        "## Part 2: load the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYg0MGIhYUdS"
      },
      "source": [
        "# First, we need to build the vocabulary so that we know how to map tokens to tensor indicies.\n",
        "# For the sake of replicating the paper results, we will load the pre-computed vocabulary objects used in the paper.\n",
        "voc = loadPrecomputedVoc(\"wikiconv\", WORD2INDEX_URL, INDEX2WORD_URL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyB85WGnIvi9"
      },
      "source": [
        "train_pairs = loadPairs(voc, corpus_train, \"train\", last_only=True)\n",
        "val_pairs = loadPairs(voc, corpus_val, \"val\", last_only=True)\n",
        "test_pairs = loadPairs(voc, corpus_test, \"test\", last_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anDPXwRGI5Er"
      },
      "source": [
        "## Part 3: define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVkNiYwPxyoC"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    \"\"\"This module represents the utterance encoder component of CRAFT, responsible for creating vector representations of utterances\"\"\"\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden\n",
        "\n",
        "class ContextEncoderRNN(nn.Module):\n",
        "    \"\"\"This module represents the context encoder component of CRAFT, responsible for creating an order-sensitive vector representation of conversation context\"\"\"\n",
        "    def __init__(self, hidden_size, n_layers=1, dropout=0):\n",
        "        super(ContextEncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # only unidirectional GRU for context encoding\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=False)\n",
        "        \n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(input_seq, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # return output and final hidden state\n",
        "        return outputs, hidden\n",
        "\n",
        "class SingleTargetClf(nn.Module):\n",
        "    \"\"\"This module represents the CRAFT classifier head, which takes the context encoding and uses it to make a forecast\"\"\"\n",
        "    def __init__(self, hidden_size, dropout=0.1):\n",
        "        super(SingleTargetClf, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # initialize classifier\n",
        "        self.layer1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer1_act = nn.LeakyReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.layer2_act = nn.LeakyReLU()\n",
        "        self.clf = nn.Linear(hidden_size // 2, 1)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, encoder_outputs, encoder_input_lengths):\n",
        "        # from stackoverflow (https://stackoverflow.com/questions/50856936/taking-the-last-state-from-bilstm-bigru-in-pytorch)\n",
        "        # First we unsqueeze seqlengths two times so it has the same number of\n",
        "        # of dimensions as output_forward\n",
        "        # (batch_size) -> (1, batch_size, 1)\n",
        "        lengths = encoder_input_lengths.unsqueeze(0).unsqueeze(2)\n",
        "        # Then we expand it accordingly\n",
        "        # (1, batch_size, 1) -> (1, batch_size, hidden_size) \n",
        "        lengths = lengths.expand((1, -1, encoder_outputs.size(2)))\n",
        "\n",
        "        # take only the last state of the encoder for each batch\n",
        "        last_outputs = torch.gather(encoder_outputs, 0, lengths-1).squeeze()\n",
        "        # forward pass through hidden layers\n",
        "        layer1_out = self.layer1_act(self.layer1(self.dropout(last_outputs)))\n",
        "        layer2_out = self.layer2_act(self.layer2(self.dropout(layer1_out)))\n",
        "        # compute and return logits\n",
        "        logits = self.clf(self.dropout(layer2_out)).squeeze()\n",
        "        return logits\n",
        "\n",
        "class Predictor(nn.Module):\n",
        "    \"\"\"This helper module encapsulates the CRAFT pipeline, defining the logic of passing an input through each consecutive sub-module.\"\"\"\n",
        "    def __init__(self, encoder, context_encoder, classifier):\n",
        "        super(Predictor, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.context_encoder = context_encoder\n",
        "        self.classifier = classifier\n",
        "        \n",
        "    def forward(self, input_batch, dialog_lengths, dialog_lengths_list, utt_lengths, batch_indices, dialog_indices, batch_size, max_length):\n",
        "        # Forward input through encoder model\n",
        "        _, utt_encoder_hidden = self.encoder(input_batch, utt_lengths)\n",
        "        \n",
        "        # Convert utterance encoder final states to batched dialogs for use by context encoder\n",
        "        context_encoder_input = makeContextEncoderInput(utt_encoder_hidden, dialog_lengths_list, batch_size, batch_indices, dialog_indices)\n",
        "        \n",
        "        # Forward pass through context encoder\n",
        "        context_encoder_outputs, context_encoder_hidden = self.context_encoder(context_encoder_input, dialog_lengths)\n",
        "        \n",
        "        # Forward pass through classifier to get prediction logits\n",
        "        logits = self.classifier(context_encoder_outputs, dialog_lengths)\n",
        "        \n",
        "        # Apply sigmoid activation\n",
        "        predictions = F.sigmoid(logits)\n",
        "        return predictions\n",
        "\n",
        "def makeContextEncoderInput(utt_encoder_hidden, dialog_lengths, batch_size, batch_indices, dialog_indices):\n",
        "    \"\"\"The utterance encoder takes in utterances in combined batches, with no knowledge of which ones go where in which conversation.\n",
        "       Its output is therefore also unordered. We correct this by using the information computed during tensor conversion to regroup\n",
        "       the utterance vectors into their proper conversational order.\"\"\"\n",
        "    # first, sum the forward and backward encoder states\n",
        "    utt_encoder_summed = utt_encoder_hidden[-2,:,:] + utt_encoder_hidden[-1,:,:]\n",
        "    # we now have hidden state of shape [utterance_batch_size, hidden_size]\n",
        "    # split it into a list of [hidden_size,] x utterance_batch_size\n",
        "    last_states = [t.squeeze() for t in utt_encoder_summed.split(1, dim=0)]\n",
        "    \n",
        "    # create a placeholder list of tensors to group the states by source dialog\n",
        "    states_dialog_batched = [[None for _ in range(dialog_lengths[i])] for i in range(batch_size)]\n",
        "    \n",
        "    # group the states by source dialog\n",
        "    for hidden_state, batch_idx, dialog_idx in zip(last_states, batch_indices, dialog_indices):\n",
        "        states_dialog_batched[batch_idx][dialog_idx] = hidden_state\n",
        "        \n",
        "    # stack each dialog into a tensor of shape [dialog_length, hidden_size]\n",
        "    states_dialog_batched = [torch.stack(d) for d in states_dialog_batched]\n",
        "    \n",
        "    # finally, condense all the dialog tensors into a single zero-padded tensor\n",
        "    # of shape [max_dialog_length, batch_size, hidden_size]\n",
        "    return torch.nn.utils.rnn.pad_sequence(states_dialog_batched)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47VXs32VI-qP"
      },
      "source": [
        "## Part 4: define training loop\n",
        "\n",
        "Now that we have all the model components defined, we need to define the actual training procedure. This will be a fairly standard neural network training loop, iterating over batches of labeled dialogs and computing cross-entropy loss on the predicted label. We will also define evaluation functions so that we can compute accuracy on the validation set after every epoch, allowing us to keep the model with the best validation performance. Note that for the sake of simpler code, validation accuracy is computed in the \"unfair\" manner using a single run of CRAFT over the full context preceding the actual personal attack, rather than the more realistic (and complicated) iterated evaluation that is used for final evaluation of the test set (in practice the two metrics track each other fairly well, making this a reasonable simplification for the sake of easy validation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcITkn3zETim"
      },
      "source": [
        "def train(input_variable, dialog_lengths, dialog_lengths_list, utt_lengths, batch_indices, dialog_indices, labels, # input/output arguments\n",
        "          encoder, context_encoder, attack_clf,                                                                    # network arguments\n",
        "          encoder_optimizer, context_encoder_optimizer, attack_clf_optimizer,                                      # optimization arguments\n",
        "          batch_size, clip, max_length=MAX_LENGTH):                                                                # misc arguments\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    context_encoder_optimizer.zero_grad()\n",
        "    attack_clf_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    dialog_lengths = dialog_lengths.to(device)\n",
        "    utt_lengths = utt_lengths.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass through utterance encoder\n",
        "    _, utt_encoder_hidden = encoder(input_variable, utt_lengths)\n",
        "    \n",
        "    # Convert utterance encoder final states to batched dialogs for use by context encoder\n",
        "    context_encoder_input = makeContextEncoderInput(utt_encoder_hidden, dialog_lengths_list, batch_size, batch_indices, dialog_indices)\n",
        "    \n",
        "    # Forward pass through context encoder\n",
        "    context_encoder_outputs, _ = context_encoder(context_encoder_input, dialog_lengths)\n",
        "\n",
        "    # Forward pass through classifier to get prediction logits\n",
        "    logits = attack_clf(context_encoder_outputs, dialog_lengths)\n",
        "    \n",
        "    # Calculate loss\n",
        "    loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
        "\n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = torch.nn.utils.clip_grad_norm_(context_encoder.parameters(), clip)\n",
        "    _ = torch.nn.utils.clip_grad_norm_(attack_clf.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    context_encoder_optimizer.step()\n",
        "    attack_clf_optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def evaluateBatch(encoder, context_encoder, predictor, voc, input_batch, dialog_lengths, \n",
        "                  dialog_lengths_list, utt_lengths, batch_indices, dialog_indices, batch_size, device, max_length=MAX_LENGTH):\n",
        "    # Set device options\n",
        "    input_batch = input_batch.to(device)\n",
        "    dialog_lengths = dialog_lengths.to(device)\n",
        "    utt_lengths = utt_lengths.to(device)\n",
        "    # Predict future attack using predictor\n",
        "    scores = predictor(input_batch, dialog_lengths, dialog_lengths_list, utt_lengths, batch_indices, dialog_indices, batch_size, max_length)\n",
        "    predictions = (scores > 0.5).float()\n",
        "    return predictions, scores\n",
        "\n",
        "def validate(dataset, encoder, context_encoder, predictor, voc, batch_size, device):\n",
        "    # create a batch iterator for the given data\n",
        "    batch_iterator = batchIterator(voc, dataset, batch_size, shuffle=False)\n",
        "    # find out how many iterations we will need to cover the whole dataset\n",
        "    n_iters = len(dataset) // batch_size + int(len(dataset) % batch_size > 0)\n",
        "    # containers for full prediction results so we can compute accuracy at the end\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for iteration in range(1, n_iters+1):\n",
        "        batch, batch_dialogs, _, true_batch_size = next(batch_iterator)\n",
        "        # Extract fields from batch\n",
        "        input_variable, dialog_lengths, utt_lengths, batch_indices, dialog_indices, labels, convo_ids, target_variable, mask, max_target_len = batch\n",
        "        dialog_lengths_list = [len(x) for x in batch_dialogs]\n",
        "        # run the model\n",
        "        predictions, scores = evaluateBatch(encoder, context_encoder, predictor, voc, input_variable,\n",
        "                                            dialog_lengths, dialog_lengths_list, utt_lengths, batch_indices, dialog_indices,\n",
        "                                            true_batch_size, device)\n",
        "        # aggregate results for computing accuracy at the end\n",
        "        all_preds += [p.item() for p in predictions]\n",
        "        all_labels += [l.item() for l in labels]\n",
        "        print(\"Iteration: {}; Percent complete: {:.1f}%\".format(iteration, iteration / n_iters * 100))\n",
        "\n",
        "    # compute and return the accuracy\n",
        "    return (np.asarray(all_preds) == np.asarray(all_labels)).mean()\n",
        "\n",
        "def trainIters(voc, pairs, val_pairs, encoder, context_encoder, attack_clf,\n",
        "               encoder_optimizer, context_encoder_optimizer, attack_clf_optimizer, embedding,\n",
        "               n_iteration, batch_size, print_every, validate_every, clip):\n",
        "    \n",
        "    # create a batch iterator for training data\n",
        "    batch_iterator = batchIterator(voc, pairs, batch_size)\n",
        "    \n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    # keep track of best validation accuracy - only save when we have a model that beats the current best\n",
        "    best_acc = 0\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch, training_dialogs, _, true_batch_size = next(batch_iterator)\n",
        "        # Extract fields from batch\n",
        "        input_variable, dialog_lengths, utt_lengths, batch_indices, dialog_indices, labels, _, target_variable, mask, max_target_len = training_batch\n",
        "        dialog_lengths_list = [len(x) for x in training_dialogs]\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, dialog_lengths, dialog_lengths_list, utt_lengths, batch_indices, dialog_indices, labels, # input/output arguments\n",
        "                     encoder, context_encoder, attack_clf,                                                                    # network arguments\n",
        "                     encoder_optimizer, context_encoder_optimizer, attack_clf_optimizer,                                      # optimization arguments\n",
        "                     true_batch_size, clip)                                                                                   # misc arguments\n",
        "        print_loss += loss\n",
        "        \n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        if (iteration % validate_every == 0):\n",
        "            print(\"Validating!\")\n",
        "            # put the network components into evaluation mode\n",
        "            encoder.eval()\n",
        "            context_encoder.eval()\n",
        "            attack_clf.eval()\n",
        "            \n",
        "            predictor = Predictor(encoder, context_encoder, attack_clf)\n",
        "            accuracy = validate(val_pairs, encoder, context_encoder, predictor, voc, batch_size, device)\n",
        "            print(\"Validation set accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "            # keep track of our best model so far\n",
        "            if accuracy > best_acc:\n",
        "                print(\"Validation accuracy better than current best; saving model...\")\n",
        "                best_acc = accuracy\n",
        "                torch.save({\n",
        "                    'iteration': iteration,\n",
        "                    'en': encoder.state_dict(),\n",
        "                    'ctx': context_encoder.state_dict(),\n",
        "                    'atk_clf': attack_clf.state_dict(),\n",
        "                    'en_opt': encoder_optimizer.state_dict(),\n",
        "                    'ctx_opt': context_encoder_optimizer.state_dict(),\n",
        "                    'atk_clf_opt': attack_clf_optimizer.state_dict(),\n",
        "                    'loss': loss,\n",
        "                    'voc_dict': voc.__dict__,\n",
        "                    'embedding': embedding.state_dict()\n",
        "                }, \"finetuned_model.tar\")\n",
        "            \n",
        "            # put the network components back into training mode\n",
        "            encoder.train()\n",
        "            context_encoder.train()\n",
        "            attack_clf.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va7Q3cIbJC5I"
      },
      "source": [
        "## Part 5: define the evaluation procedure\n",
        "\n",
        "We're almost ready to run! The last component we need is some code to evaluate performance on the test set after fine-tuning is completed. This evaluation should use the full iterative procedure described in the paper, replicating how a system might be deployed in practice, without knowledge of where the personal attack occurs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjAbDicxEYMw"
      },
      "source": [
        "def evaluateDataset(dataset, encoder, context_encoder, predictor, voc, batch_size, device):\n",
        "    # create a batch iterator for the given data\n",
        "    batch_iterator = batchIterator(voc, dataset, batch_size, shuffle=False)\n",
        "    # find out how many iterations we will need to cover the whole dataset\n",
        "    n_iters = len(dataset) // batch_size + int(len(dataset) % batch_size > 0)\n",
        "    output_df = {\n",
        "        \"id\": [],\n",
        "        \"prediction\": [],\n",
        "        \"score\": []\n",
        "    }\n",
        "    for iteration in range(1, n_iters+1):\n",
        "        batch, batch_dialogs, _, true_batch_size = next(batch_iterator)\n",
        "        # Extract fields from batch\n",
        "        input_variable, dialog_lengths, utt_lengths, batch_indices, dialog_indices, labels, convo_ids, target_variable, mask, max_target_len = batch\n",
        "        dialog_lengths_list = [len(x) for x in batch_dialogs]\n",
        "        # run the model\n",
        "        predictions, scores = evaluateBatch(encoder, context_encoder, predictor, voc, input_variable,\n",
        "                                            dialog_lengths, dialog_lengths_list, utt_lengths, batch_indices, dialog_indices,\n",
        "                                            true_batch_size, device)\n",
        "\n",
        "        # format the output as a dataframe (which we can later re-join with the corpus)\n",
        "        for i in range(true_batch_size):\n",
        "            convo_id = convo_ids[i]\n",
        "            pred = predictions[i].item()\n",
        "            score = scores[i].item()\n",
        "            output_df[\"id\"].append(convo_id)\n",
        "            output_df[\"prediction\"].append(pred)\n",
        "            output_df[\"score\"].append(score)\n",
        "                \n",
        "        print(\"Iteration: {}; Percent complete: {:.1f}%\".format(iteration, iteration / n_iters * 100))\n",
        "\n",
        "    return pd.DataFrame(output_df).set_index(\"id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnDN42gSJKMV"
      },
      "source": [
        "## Part 6: build and fine-tune the model\n",
        "\n",
        "We finally have all the components we need! Now we can instantiate the CRAFT model components, load the pre-trained weights, and run fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYtuBDeWEdAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae7fadb-8f7e-4011-a2a2-3dfae21b6861"
      },
      "source": [
        "# Fix random state (affect native Python code only, does not affect PyTorch and hence does not guarantee reproducibility)\n",
        "random.seed(2019)\n",
        "\n",
        "# Tell torch to use GPU. Note that if you are running this notebook in a non-GPU environment, you can change 'cuda' to 'cpu' to get the code to run.\n",
        "device = torch.device('cpu')\n",
        "\n",
        "print(\"Loading saved parameters...\")\n",
        "if not os.path.isfile(\"pretrained_model.tar\"):\n",
        "    print(\"\\tDownloading pre-trained CRAFT...\")\n",
        "    urlretrieve(MODEL_URL, \"pretrained_model.tar\")\n",
        "    print(\"\\t...Done!\")\n",
        "checkpoint = torch.load(\"pretrained_model.tar\")\n",
        "# If running in a non-GPU environment, you need to tell PyTorch to convert the parameters to CPU tensor format.\n",
        "# To do so, replace the previous line with the following:\n",
        "checkpoint = torch.load(\"pretrained_model.tar\", map_location=torch.device('cpu'))\n",
        "encoder_sd = checkpoint['en']\n",
        "context_sd = checkpoint['ctx']\n",
        "embedding_sd = checkpoint['embedding']\n",
        "voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "print('Building encoders, decoder, and classifier...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "embedding.load_state_dict(embedding_sd)\n",
        "# Initialize utterance and context encoders\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "context_encoder = ContextEncoderRNN(hidden_size, context_encoder_n_layers, dropout)\n",
        "encoder.load_state_dict(encoder_sd)\n",
        "context_encoder.load_state_dict(context_sd)\n",
        "# Initialize classifier\n",
        "attack_clf = SingleTargetClf(hidden_size, dropout)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "context_encoder = context_encoder.to(device)\n",
        "attack_clf = attack_clf.to(device)\n",
        "print('Models built and ready to go!')\n",
        "\n",
        "# Compute the number of training iterations we will need in order to achieve the number of epochs specified in the settings at the start of the notebook\n",
        "n_iter_per_epoch = len(train_pairs) // batch_size + int(len(train_pairs) % batch_size == 1)\n",
        "n_iteration = n_iter_per_epoch * train_epochs\n",
        "\n",
        "# Put dropout layers in train mode\n",
        "encoder.train()\n",
        "context_encoder.train()\n",
        "attack_clf.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "context_encoder_optimizer = optim.Adam(context_encoder.parameters(), lr=learning_rate)\n",
        "attack_clf_optimizer = optim.Adam(attack_clf.parameters(), lr=learning_rate)\n",
        "\n",
        "# Run training iterations, validating after every epoch\n",
        "print(\"Starting Training!\")\n",
        "print(\"Will train for {} iterations\".format(n_iteration))\n",
        "trainIters(voc, train_pairs, val_pairs, encoder, context_encoder, attack_clf,\n",
        "           encoder_optimizer, context_encoder_optimizer, attack_clf_optimizer, embedding,\n",
        "           n_iteration, batch_size, print_every, n_iter_per_epoch, clip)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading saved parameters...\n",
            "Building encoders, decoder, and classifier...\n",
            "Models built and ready to go!\n",
            "Building optimizers...\n",
            "Starting Training!\n",
            "Will train for 130 iterations\n",
            "Initializing ...\n",
            "Training...\n",
            "Iteration: 10; Percent complete: 7.7%; Average loss: 0.6934\n",
            "Iteration: 20; Percent complete: 15.4%; Average loss: 0.6926\n",
            "Validating!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 1; Percent complete: 11.1%\n",
            "Iteration: 2; Percent complete: 22.2%\n",
            "Iteration: 3; Percent complete: 33.3%\n",
            "Iteration: 4; Percent complete: 44.4%\n",
            "Iteration: 5; Percent complete: 55.6%\n",
            "Iteration: 6; Percent complete: 66.7%\n",
            "Iteration: 7; Percent complete: 77.8%\n",
            "Iteration: 8; Percent complete: 88.9%\n",
            "Iteration: 9; Percent complete: 100.0%\n",
            "Validation set accuracy: 75.05%\n",
            "Validation accuracy better than current best; saving model...\n",
            "Iteration: 30; Percent complete: 23.1%; Average loss: 0.6917\n",
            "Iteration: 40; Percent complete: 30.8%; Average loss: 0.6908\n",
            "Iteration: 50; Percent complete: 38.5%; Average loss: 0.6902\n",
            "Validating!\n",
            "Iteration: 1; Percent complete: 11.1%\n",
            "Iteration: 2; Percent complete: 22.2%\n",
            "Iteration: 3; Percent complete: 33.3%\n",
            "Iteration: 4; Percent complete: 44.4%\n",
            "Iteration: 5; Percent complete: 55.6%\n",
            "Iteration: 6; Percent complete: 66.7%\n",
            "Iteration: 7; Percent complete: 77.8%\n",
            "Iteration: 8; Percent complete: 88.9%\n",
            "Iteration: 9; Percent complete: 100.0%\n",
            "Validation set accuracy: 71.77%\n",
            "Iteration: 60; Percent complete: 46.2%; Average loss: 0.6886\n",
            "Iteration: 70; Percent complete: 53.8%; Average loss: 0.6866\n",
            "Validating!\n",
            "Iteration: 1; Percent complete: 11.1%\n",
            "Iteration: 2; Percent complete: 22.2%\n",
            "Iteration: 3; Percent complete: 33.3%\n",
            "Iteration: 4; Percent complete: 44.4%\n",
            "Iteration: 5; Percent complete: 55.6%\n",
            "Iteration: 6; Percent complete: 66.7%\n",
            "Iteration: 7; Percent complete: 77.8%\n",
            "Iteration: 8; Percent complete: 88.9%\n",
            "Iteration: 9; Percent complete: 100.0%\n",
            "Validation set accuracy: 69.22%\n",
            "Iteration: 80; Percent complete: 61.5%; Average loss: 0.6845\n",
            "Iteration: 90; Percent complete: 69.2%; Average loss: 0.6814\n",
            "Iteration: 100; Percent complete: 76.9%; Average loss: 0.6754\n",
            "Validating!\n",
            "Iteration: 1; Percent complete: 11.1%\n",
            "Iteration: 2; Percent complete: 22.2%\n",
            "Iteration: 3; Percent complete: 33.3%\n",
            "Iteration: 4; Percent complete: 44.4%\n",
            "Iteration: 5; Percent complete: 55.6%\n",
            "Iteration: 6; Percent complete: 66.7%\n",
            "Iteration: 7; Percent complete: 77.8%\n",
            "Iteration: 8; Percent complete: 88.9%\n",
            "Iteration: 9; Percent complete: 100.0%\n",
            "Validation set accuracy: 67.03%\n",
            "Iteration: 110; Percent complete: 84.6%; Average loss: 0.6702\n",
            "Iteration: 120; Percent complete: 92.3%; Average loss: 0.6572\n",
            "Iteration: 130; Percent complete: 100.0%; Average loss: 0.6433\n",
            "Validating!\n",
            "Iteration: 1; Percent complete: 11.1%\n",
            "Iteration: 2; Percent complete: 22.2%\n",
            "Iteration: 3; Percent complete: 33.3%\n",
            "Iteration: 4; Percent complete: 44.4%\n",
            "Iteration: 5; Percent complete: 55.6%\n",
            "Iteration: 6; Percent complete: 66.7%\n",
            "Iteration: 7; Percent complete: 77.8%\n",
            "Iteration: 8; Percent complete: 88.9%\n",
            "Iteration: 9; Percent complete: 100.0%\n",
            "Validation set accuracy: 64.30%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE8tDaM7JPSR"
      },
      "source": [
        "## Part 7: run test set evaluation\n",
        "\n",
        "Now that we have successfully fine-tuned the model, we run it on the test set so that we can evaluate performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-267DgUbZEDe",
        "outputId": "876dcf82-dbba-467e-a8f8-c122af81cbdb"
      },
      "source": [
        "# Fix random state for reproducibility\n",
        "random.seed(2019)\n",
        "\n",
        "# Tell torch to use GPU. Note that if you are running this notebook in a non-GPU environment, you can change 'cuda' to 'cpu' to get the code to run.\n",
        "device = torch.device('cpu')\n",
        "\n",
        "print(\"Loading saved parameters...\")\n",
        "checkpoint = torch.load(\"finetuned_model.tar\")\n",
        "# If running in a non-GPU environment, you need to tell PyTorch to convert the parameters to CPU tensor format.\n",
        "# To do so, replace the previous line with the following:\n",
        "#checkpoint = torch.load(\"model.tar\", map_location=torch.device('cpu'))\n",
        "encoder_sd = checkpoint['en']\n",
        "context_sd = checkpoint['ctx']\n",
        "attack_clf_sd = checkpoint['atk_clf']\n",
        "embedding_sd = checkpoint['embedding']\n",
        "voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "print('Building encoders, decoder, and classifier...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "embedding.load_state_dict(embedding_sd)\n",
        "# Initialize utterance and context encoders\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "context_encoder = ContextEncoderRNN(hidden_size, context_encoder_n_layers, dropout)\n",
        "encoder.load_state_dict(encoder_sd)\n",
        "context_encoder.load_state_dict(context_sd)\n",
        "# Initialize classifier\n",
        "attack_clf = SingleTargetClf(hidden_size, dropout)\n",
        "attack_clf.load_state_dict(attack_clf_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "context_encoder = context_encoder.to(device)\n",
        "attack_clf = attack_clf.to(device)\n",
        "print('Models built and ready to go!')\n",
        "\n",
        "# Set dropout layers to eval mode\n",
        "encoder.eval()\n",
        "context_encoder.eval()\n",
        "attack_clf.eval()\n",
        "\n",
        "# Initialize the pipeline\n",
        "predictor = Predictor(encoder, context_encoder, attack_clf)\n",
        "\n",
        "# Run the pipeline!\n",
        "forecasts_df = evaluateDataset(test_pairs, encoder, context_encoder, predictor, voc, batch_size, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading saved parameters...\n",
            "Building encoders, decoder, and classifier...\n",
            "Models built and ready to go!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 1; Percent complete: 10.0%\n",
            "Iteration: 2; Percent complete: 20.0%\n",
            "Iteration: 3; Percent complete: 30.0%\n",
            "Iteration: 4; Percent complete: 40.0%\n",
            "Iteration: 5; Percent complete: 50.0%\n",
            "Iteration: 6; Percent complete: 60.0%\n",
            "Iteration: 7; Percent complete: 70.0%\n",
            "Iteration: 8; Percent complete: 80.0%\n",
            "Iteration: 9; Percent complete: 90.0%\n",
            "Iteration: 10; Percent complete: 100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU85ijckaNFP"
      },
      "source": [
        "\n",
        "# Inspect some of the outputs as a sanity-check\n",
        "test_predictions_labels = forecasts_df.prediction\n",
        "test_pos_probs = forecasts_df.score\n",
        "test_true_labels = pd.DataFrame(test_pairs)[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoB4ay1RS3an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857506f8-bd50-4134-8b58-089e9f5a3c3a"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "labels_ids = {'neg': 0, 'pos': 1}\n",
        "# Create the evaluation report.\n",
        "evaluation_report = classification_report(test_true_labels, test_predictions_labels, labels=list(labels_ids.values()), target_names=list(labels_ids.keys()))\n",
        "# Show the evaluation report.\n",
        "print(evaluation_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.85      0.82      0.83       434\n",
            "         pos       0.52      0.57      0.54       148\n",
            "\n",
            "    accuracy                           0.76       582\n",
            "   macro avg       0.68      0.69      0.69       582\n",
            "weighted avg       0.76      0.76      0.76       582\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "2dJwBYTrGNJH",
        "outputId": "c2d5143f-6064-44a6-dda5-6a53e1ef8d89"
      },
      "source": [
        "\n",
        "\n",
        "cm = confusion_matrix(test_true_labels, test_predictions_labels,\n",
        "                      normalize = 'true')\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(cm)\n",
        "plt.title('Confusion matrix of the classifier')\n",
        "fig.colorbar(cax)\n",
        "ax.set_xticklabels([''] + ['pos', 'neg'])\n",
        "ax.set_yticklabels([''] + ['pos', 'neg'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "test_prec, test_rec, _ = precision_recall_curve(test_true_labels, test_pos_probs)\n",
        "test_pr_auc = auc(test_rec, test_prec)\n",
        "\n",
        "test_acc = accuracy_score(test_true_labels, test_predictions_labels)\n",
        "print('')\n",
        "print('Test Metrics')\n",
        "print('Accuracy Score: {:.3f}'.format(test_acc))\n",
        "print('PR AUC Score: {:.3f}'.format(test_pr_auc))\n",
        "print('')\n",
        "\n",
        "# plot the no skill precision-recall curve\n",
        "#pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
        "\n",
        "from matplotlib import pyplot\n",
        "# plot the model precision-recall curve\n",
        "pyplot.plot(test_rec, test_prec, marker='.', label='PR AUC Score: {:.2f}'.format(test_pr_auc))\n",
        "# axis labels\n",
        "pyplot.xlabel('Recall')\n",
        "pyplot.ylabel('Precision')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test Metrics\n",
            "Accuracy Score: 0.756\n",
            "PR AUC Score: 0.548\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8KF06f-HfrU",
        "outputId": "47e26017-97c4-4a38-9416-1952e2ba2148"
      },
      "source": [
        "test_rec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.        , 0.99324324, 0.99324324, 0.99324324, 0.99324324,\n",
              "       0.99324324, 0.99324324, 0.99324324, 0.99324324, 0.99324324,\n",
              "       0.99324324, 0.99324324, 0.99324324, 0.99324324, 0.99324324,\n",
              "       0.99324324, 0.99324324, 0.98648649, 0.98648649, 0.98648649,\n",
              "       0.98648649, 0.98648649, 0.98648649, 0.98648649, 0.97972973,\n",
              "       0.97297297, 0.97297297, 0.97297297, 0.97297297, 0.97297297,\n",
              "       0.97297297, 0.97297297, 0.97297297, 0.97297297, 0.97297297,\n",
              "       0.97297297, 0.97297297, 0.97297297, 0.97297297, 0.97297297,\n",
              "       0.97297297, 0.97297297, 0.97297297, 0.97297297, 0.97297297,\n",
              "       0.97297297, 0.97297297, 0.97297297, 0.97297297, 0.97297297,\n",
              "       0.97297297, 0.97297297, 0.97297297, 0.97297297, 0.97297297,\n",
              "       0.97297297, 0.97297297, 0.97297297, 0.97297297, 0.97297297,\n",
              "       0.97297297, 0.97297297, 0.97297297, 0.97297297, 0.97297297,\n",
              "       0.97297297, 0.97297297, 0.97297297, 0.97297297, 0.97297297,\n",
              "       0.97297297, 0.97297297, 0.96621622, 0.96621622, 0.96621622,\n",
              "       0.96621622, 0.95945946, 0.95945946, 0.95945946, 0.95945946,\n",
              "       0.95945946, 0.95945946, 0.95945946, 0.95945946, 0.9527027 ,\n",
              "       0.9527027 , 0.9527027 , 0.94594595, 0.94594595, 0.94594595,\n",
              "       0.94594595, 0.93918919, 0.93918919, 0.93918919, 0.93918919,\n",
              "       0.93918919, 0.93918919, 0.93918919, 0.93918919, 0.93918919,\n",
              "       0.93918919, 0.93918919, 0.93918919, 0.93918919, 0.93918919,\n",
              "       0.93918919, 0.93918919, 0.93918919, 0.93918919, 0.93918919,\n",
              "       0.93918919, 0.93918919, 0.93918919, 0.93918919, 0.93918919,\n",
              "       0.93918919, 0.93918919, 0.93918919, 0.93918919, 0.93918919,\n",
              "       0.93918919, 0.93918919, 0.93918919, 0.93918919, 0.93918919,\n",
              "       0.93918919, 0.93918919, 0.93918919, 0.93918919, 0.93243243,\n",
              "       0.93243243, 0.93243243, 0.93243243, 0.93243243, 0.93243243,\n",
              "       0.93243243, 0.93243243, 0.93243243, 0.93243243, 0.93243243,\n",
              "       0.93243243, 0.93243243, 0.93243243, 0.93243243, 0.92567568,\n",
              "       0.92567568, 0.92567568, 0.92567568, 0.92567568, 0.92567568,\n",
              "       0.92567568, 0.91891892, 0.91891892, 0.91891892, 0.91891892,\n",
              "       0.91891892, 0.91216216, 0.91216216, 0.91216216, 0.90540541,\n",
              "       0.90540541, 0.90540541, 0.90540541, 0.89864865, 0.89864865,\n",
              "       0.89189189, 0.89189189, 0.89189189, 0.89189189, 0.89189189,\n",
              "       0.88513514, 0.88513514, 0.88513514, 0.88513514, 0.88513514,\n",
              "       0.88513514, 0.88513514, 0.87837838, 0.87837838, 0.87837838,\n",
              "       0.87837838, 0.87837838, 0.87837838, 0.87837838, 0.87162162,\n",
              "       0.87162162, 0.87162162, 0.87162162, 0.87162162, 0.87162162,\n",
              "       0.87162162, 0.87162162, 0.87162162, 0.87162162, 0.87162162,\n",
              "       0.87162162, 0.86486486, 0.86486486, 0.85810811, 0.85810811,\n",
              "       0.85810811, 0.85810811, 0.85810811, 0.85810811, 0.85810811,\n",
              "       0.85810811, 0.85810811, 0.85810811, 0.85810811, 0.85135135,\n",
              "       0.85135135, 0.85135135, 0.85135135, 0.85135135, 0.84459459,\n",
              "       0.84459459, 0.84459459, 0.84459459, 0.84459459, 0.83783784,\n",
              "       0.83783784, 0.83783784, 0.83783784, 0.83783784, 0.83783784,\n",
              "       0.83783784, 0.83783784, 0.83783784, 0.83783784, 0.83783784,\n",
              "       0.83108108, 0.83108108, 0.83108108, 0.83108108, 0.82432432,\n",
              "       0.82432432, 0.82432432, 0.82432432, 0.82432432, 0.82432432,\n",
              "       0.82432432, 0.82432432, 0.82432432, 0.82432432, 0.82432432,\n",
              "       0.81756757, 0.81756757, 0.81756757, 0.81756757, 0.81081081,\n",
              "       0.81081081, 0.80405405, 0.80405405, 0.80405405, 0.7972973 ,\n",
              "       0.79054054, 0.79054054, 0.79054054, 0.79054054, 0.79054054,\n",
              "       0.79054054, 0.79054054, 0.79054054, 0.78378378, 0.77702703,\n",
              "       0.77702703, 0.77702703, 0.77702703, 0.77027027, 0.76351351,\n",
              "       0.76351351, 0.76351351, 0.76351351, 0.76351351, 0.76351351,\n",
              "       0.76351351, 0.76351351, 0.75675676, 0.75675676, 0.75      ,\n",
              "       0.75      , 0.74324324, 0.74324324, 0.74324324, 0.74324324,\n",
              "       0.74324324, 0.74324324, 0.74324324, 0.74324324, 0.74324324,\n",
              "       0.74324324, 0.73648649, 0.73648649, 0.73648649, 0.73648649,\n",
              "       0.73648649, 0.73648649, 0.72972973, 0.72972973, 0.72972973,\n",
              "       0.72297297, 0.72297297, 0.72297297, 0.72297297, 0.72297297,\n",
              "       0.72297297, 0.72297297, 0.72297297, 0.72297297, 0.72297297,\n",
              "       0.72297297, 0.72297297, 0.72297297, 0.72297297, 0.71621622,\n",
              "       0.71621622, 0.71621622, 0.71621622, 0.71621622, 0.70945946,\n",
              "       0.70945946, 0.70945946, 0.70945946, 0.7027027 , 0.7027027 ,\n",
              "       0.7027027 , 0.7027027 , 0.7027027 , 0.7027027 , 0.7027027 ,\n",
              "       0.7027027 , 0.7027027 , 0.7027027 , 0.7027027 , 0.7027027 ,\n",
              "       0.7027027 , 0.7027027 , 0.69594595, 0.68918919, 0.68918919,\n",
              "       0.68918919, 0.68243243, 0.67567568, 0.66891892, 0.66891892,\n",
              "       0.66891892, 0.66891892, 0.66891892, 0.66891892, 0.66891892,\n",
              "       0.66891892, 0.66891892, 0.66216216, 0.66216216, 0.66216216,\n",
              "       0.66216216, 0.66216216, 0.66216216, 0.66216216, 0.66216216,\n",
              "       0.66216216, 0.66216216, 0.66216216, 0.66216216, 0.66216216,\n",
              "       0.66216216, 0.66216216, 0.65540541, 0.65540541, 0.65540541,\n",
              "       0.64864865, 0.64864865, 0.64864865, 0.64864865, 0.64864865,\n",
              "       0.64864865, 0.64864865, 0.64189189, 0.64189189, 0.64189189,\n",
              "       0.64189189, 0.63513514, 0.62837838, 0.62837838, 0.62162162,\n",
              "       0.62162162, 0.61486486, 0.61486486, 0.61486486, 0.61486486,\n",
              "       0.61486486, 0.61486486, 0.61486486, 0.60810811, 0.60810811,\n",
              "       0.60135135, 0.60135135, 0.60135135, 0.60135135, 0.60135135,\n",
              "       0.60135135, 0.60135135, 0.59459459, 0.59459459, 0.59459459,\n",
              "       0.58783784, 0.58783784, 0.58108108, 0.58108108, 0.58108108,\n",
              "       0.57432432, 0.57432432, 0.57432432, 0.57432432, 0.57432432,\n",
              "       0.57432432, 0.57432432, 0.56756757, 0.56756757, 0.56756757,\n",
              "       0.56081081, 0.55405405, 0.5472973 , 0.54054054, 0.54054054,\n",
              "       0.54054054, 0.53378378, 0.53378378, 0.53378378, 0.52702703,\n",
              "       0.52702703, 0.52702703, 0.52702703, 0.52702703, 0.52702703,\n",
              "       0.52702703, 0.52027027, 0.51351351, 0.50675676, 0.50675676,\n",
              "       0.50675676, 0.50675676, 0.50675676, 0.50675676, 0.50675676,\n",
              "       0.5       , 0.5       , 0.5       , 0.49324324, 0.49324324,\n",
              "       0.49324324, 0.48648649, 0.47972973, 0.47972973, 0.47972973,\n",
              "       0.47297297, 0.46621622, 0.45945946, 0.4527027 , 0.44594595,\n",
              "       0.44594595, 0.44594595, 0.44594595, 0.44594595, 0.43918919,\n",
              "       0.43918919, 0.43918919, 0.43243243, 0.43243243, 0.42567568,\n",
              "       0.41891892, 0.41891892, 0.41216216, 0.40540541, 0.40540541,\n",
              "       0.40540541, 0.40540541, 0.39864865, 0.39864865, 0.39189189,\n",
              "       0.38513514, 0.38513514, 0.37837838, 0.37162162, 0.37162162,\n",
              "       0.36486486, 0.36486486, 0.35810811, 0.35810811, 0.35810811,\n",
              "       0.35135135, 0.35135135, 0.35135135, 0.34459459, 0.34459459,\n",
              "       0.34459459, 0.33783784, 0.33783784, 0.33108108, 0.32432432,\n",
              "       0.31756757, 0.31756757, 0.31756757, 0.31081081, 0.31081081,\n",
              "       0.31081081, 0.30405405, 0.2972973 , 0.29054054, 0.29054054,\n",
              "       0.28378378, 0.28378378, 0.28378378, 0.28378378, 0.28378378,\n",
              "       0.28378378, 0.28378378, 0.28378378, 0.27702703, 0.27702703,\n",
              "       0.27027027, 0.27027027, 0.26351351, 0.25675676, 0.25      ,\n",
              "       0.24324324, 0.24324324, 0.24324324, 0.23648649, 0.22972973,\n",
              "       0.22297297, 0.21621622, 0.21621622, 0.20945946, 0.20945946,\n",
              "       0.20945946, 0.2027027 , 0.19594595, 0.18918919, 0.18918919,\n",
              "       0.18918919, 0.18243243, 0.18243243, 0.17567568, 0.17567568,\n",
              "       0.16891892, 0.16216216, 0.15540541, 0.14864865, 0.14864865,\n",
              "       0.14189189, 0.14189189, 0.14189189, 0.13513514, 0.12837838,\n",
              "       0.12162162, 0.12162162, 0.11486486, 0.10810811, 0.10135135,\n",
              "       0.09459459, 0.08783784, 0.08108108, 0.07432432, 0.07432432,\n",
              "       0.06756757, 0.06756757, 0.06081081, 0.05405405, 0.0472973 ,\n",
              "       0.04054054, 0.03378378, 0.03378378, 0.02702703, 0.02027027,\n",
              "       0.01351351, 0.00675676, 0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKgXBso8_sLp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}